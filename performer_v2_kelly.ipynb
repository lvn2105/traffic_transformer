{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70yJX_h9yA-F",
        "outputId": "b7c0e01a-3bcc-4bf1-c922-33829c43e521"
      },
      "outputs": [],
      "source": [
        "#setup \n",
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "!pip install scikit-learn\n",
        "import sklearn\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (4, 3)\n",
        "mpl.rcParams['axes.grid'] = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnPnRoIYyeOy"
      },
      "outputs": [],
      "source": [
        "new_table = pd.read_csv('https://raw.githubusercontent.com/giobbu/App_Traff_Forecast_DeapLearn/master/data/Flow_BEL_street_30min.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0JEA8J30c5D",
        "outputId": "1f5c650e-bca4-42a7-fe4b-37e63ab482bc"
      },
      "outputs": [],
      "source": [
        "# this source: \n",
        "# the https://www.kaggle.com/code/giobbu/seasonal-persistence-model/notebook#Results-Comparison-between-Seasonal-model-(baseline)-and-LSTM-encoder-decoder-model\n",
        "# selects the roads that have an average traffic flow of 10 or larger, we also do this, this code is from the notebook listed above \n",
        "\n",
        "table_index = new_table.iloc[:,1:]\n",
        "ALL_STREETS = list(table_index.columns.values)\n",
        "\n",
        "mean_flow =[]\n",
        "new_street=[]\n",
        "\n",
        "mean_value = 20\n",
        "\n",
        "for street in ALL_STREETS:\n",
        "    \n",
        "    single_street=table_index[street]\n",
        "    mean = np.mean(single_street)\n",
        "    mean_flow.append(mean)\n",
        "    new_street.append(street)\n",
        "    \n",
        "    \n",
        "df_mean_flow = pd.DataFrame({'street_index':new_street, 'mean_flow': mean_flow})\n",
        "print('')\n",
        "print(df_mean_flow.head())\n",
        "print('')\n",
        "\n",
        "STREETS = df_mean_flow[(df_mean_flow['mean_flow'] >= mean_value)] \n",
        "STREETS = STREETS.sort_values(by=['street_index'])\n",
        "STREETS = list(STREETS.street_index)\n",
        "\n",
        "keys_dim = 1683\n",
        "keys_dim_time = 1686\n",
        "\n",
        "print('considering a average traffic flow of ' + str(mean_value)+' per street')\n",
        "print('')\n",
        "print('mean traffic flow '+str(mean_value)+ ' ---> number of street segments: ' + str(len(STREETS)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8PW_uZbWlBB"
      },
      "outputs": [],
      "source": [
        "new_table['datetime'] = pd.to_datetime(new_table['datetime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ZfYYEjq-W7Rv",
        "outputId": "9d0ba1ca-25dc-4d17-da56-b477916763d2"
      },
      "outputs": [],
      "source": [
        "plt.plot(new_table['datetime'],new_table.mean(axis = 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSCCJ46kYnfL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_array_ops import fake_quant_with_min_max_vars_per_channel\n",
        "#we need to create \"windows\" in the data --> \n",
        "#X: input tensor : use a window of size s, lets say 5, as X\n",
        "#Y: output vector : traffic for hour six predicted based on the previous window \n",
        "\n",
        "#this is the window function for multiple input features and multiple output pairs\n",
        "#what's the difference between df_to_x_y2 and df_to_x_y --> the first one is just for one input vector (ie the data from one street) this \n",
        "#version accepts many columns of data (all the street values and the transformed time data)\n",
        "def df_to_x_y2(positional_data,non_positional, window_size): \n",
        "  #df_as_np = df.to_numpy()\n",
        "  print(positional_data.shape)\n",
        "  print(non_positional.shape)\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range((len(positional_data))-window_size-1):\n",
        "    row = [r for r in positional_data[i:i+window_size]]\n",
        "    \n",
        "    #The line below makes the last value in the batch 0 (relative to what we are predicting, \n",
        "    #traffic flow rate, the positional information is still encoded )\n",
        "    row.append(positional_data[i+window_size+1] - non_positional[i+window_size+1]) \n",
        "    X.append(row)\n",
        "    \n",
        "    label = [non_positional[i+window_size][0:]] \n",
        "    Y.append(label)\n",
        "  return np.array(X), np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9ddvo5P3o_p"
      },
      "outputs": [],
      "source": [
        "#this is for transforming datetime to recognizable inputs \n",
        "#sin and cos transformation for hour of the day \n",
        "df = new_table\n",
        "df = df[STREETS + ['datetime']] \n",
        " #this removes all of the streets with aveage flow <10 \n",
        "\n",
        "df.head()\n",
        "\n",
        "time = df['datetime'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwhYU7pOk63h"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(df):\n",
        "  import datetime as dt\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  df = pd.Series(df)\n",
        "  hour = df.dt.hour.to_numpy().astype('float32')\n",
        "  hour_sin = np.sin(hour*2* np.pi/23)\n",
        "  hour_cos = np.cos(hour*2* np.pi/23)\n",
        "  \n",
        "  day_of_week = df.dt.dayofweek.to_numpy()\n",
        "  day_of_week_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "  day_of_week = day_of_week.reshape(-1, 1)\n",
        "  day_of_week = day_of_week_scaler.fit_transform(day_of_week)\n",
        "  day_of_week = day_of_week[:,0]\n",
        "  \n",
        "  pos_encoding_val = hour_sin + hour_cos + day_of_week\n",
        "  pos_encoding_val = pos_encoding_val.reshape(-1,1)\n",
        "  final_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  pos_encoding_val = final_scaler.fit_transform(pos_encoding_val)\n",
        "  return pos_encoding_val\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJu55qlMeoKk",
        "outputId": "3b74968b-7e71-4623-fd3d-ace4ce504d20"
      },
      "outputs": [],
      "source": [
        "#test positoinal_encoding function \n",
        "positional_encoding(time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_q9uHmeoKk"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, date_time_info,time_weight):\n",
        "    super().__init__()\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    self.time_weight = time_weight\n",
        "    self.scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    self.pos_encoding = positional_encoding(date_time_info)\n",
        "\n",
        "  def call(self, x):\n",
        "    print(x.shape)\n",
        "    print(type(x))\n",
        "    print(x)\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.scaler.fit_transform(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    print(self.pos_encoding.shape)\n",
        "    x_positioned = np.add(x*(1-self.time_weight), (self.time_weight * self.pos_encoding))\n",
        "    print(x.shape)\n",
        "    x_final,y_final= df_to_x_y2(x_positioned,x,window_size=12) \n",
        "    x_final = tf.constant(x_final)\n",
        "    y_final = tf.constant(y_final)\n",
        "    return x_final,y_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fYBUG1beoKk",
        "outputId": "4910d75b-c7f9-4235-95bb-8b70602bab55"
      },
      "outputs": [],
      "source": [
        "#testing the positional embedding class \n",
        "positional_em = PositionalEmbedding(time,0.4)\n",
        "positional_em2 = PositionalEmbedding(time,0)\n",
        "\n",
        "print('shape1',positional_em(df[STREETS].to_numpy())[0][0])\n",
        "print('shape2',positional_em2(df[STREETS].to_numpy())[0][0])\n",
        "\n",
        "x1_df = df[STREETS + ['datetime']].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XPNQuM0wfRTr"
      },
      "outputs": [],
      "source": [
        "#@title util.py \n",
        "# coding=utf-8\n",
        "# Copyright 2022 The Google Research Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Keras-based einsum layer.\n",
        "Copied from\n",
        "https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/dense_einsum.py.\n",
        "\"\"\"\n",
        "# pylint: disable=g-classes-have-attributes\n",
        "\n",
        "_CHR_IDX = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\"]\n",
        "\n",
        "\n",
        "#@tf.keras.utils.register_keras_serializable(package=\"Text\")\n",
        "class DenseEinsum(tf.keras.layers.Layer):\n",
        "  \"\"\"A densely connected layer that uses tf.einsum as the backing computation.\n",
        "  This layer can perform einsum calculations of arbitrary dimensionality.\n",
        "  Arguments:\n",
        "    output_shape: Positive integer or tuple, dimensionality of the output space.\n",
        "    num_summed_dimensions: The number of dimensions to sum over. Standard 2D\n",
        "      matmul should use 1, 3D matmul should use 2, and so forth.\n",
        "    activation: Activation function to use. If you don't specify anything, no\n",
        "      activation is applied\n",
        "      (ie. \"linear\" activation: `a(x) = x`).\n",
        "    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
        "    bias_initializer: Initializer for the bias vector.\n",
        "    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
        "      matrix.\n",
        "    bias_regularizer: Regularizer function applied to the bias vector.\n",
        "    activity_regularizer: Regularizer function applied to the output of the\n",
        "      layer (its \"activation\")..\n",
        "    kernel_constraint: Constraint function applied to the `kernel` weights\n",
        "      matrix.\n",
        "    bias_constraint: Constraint function applied to the bias vector.\n",
        "  Input shape:\n",
        "    N-D tensor with shape: `(batch_size, ..., input_dim)`. The most common\n",
        "      situation would be a 2D input with shape `(batch_size, input_dim)`.\n",
        "  Output shape:\n",
        "    N-D tensor with shape: `(batch_size, ..., units)`. For instance, for a 2D\n",
        "      input with shape `(batch_size, input_dim)`, the output would have shape\n",
        "      `(batch_size, units)`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               output_shape,\n",
        "               num_summed_dimensions=1,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer=\"glorot_uniform\",\n",
        "               bias_initializer=\"zeros\",\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "    super(DenseEinsum, self).__init__(**kwargs)\n",
        "    self._output_shape = output_shape if isinstance(\n",
        "        output_shape, (list, tuple)) else (output_shape,)\n",
        "    self._activation = tf.keras.activations.get(activation)\n",
        "    self._use_bias = use_bias\n",
        "    self._kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
        "    self._bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
        "    self._kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n",
        "    self._bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n",
        "    self._kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
        "    self._bias_constraint = tf.keras.constraints.get(bias_constraint)\n",
        "    self._num_summed_dimensions = num_summed_dimensions\n",
        "    self._einsum_string = None\n",
        "\n",
        "  def _build_einsum_string(self, free_input_dims, bound_dims, output_dims):\n",
        "    input_str = \"\"\n",
        "    kernel_str = \"\"\n",
        "    output_str = \"\"\n",
        "    letter_offset = 0\n",
        "    for i in range(free_input_dims):\n",
        "      char = _CHR_IDX[i + letter_offset]\n",
        "      input_str += char\n",
        "      output_str += char\n",
        "\n",
        "    letter_offset += free_input_dims\n",
        "    for i in range(bound_dims):\n",
        "      char = _CHR_IDX[i + letter_offset]\n",
        "      input_str += char\n",
        "      kernel_str += char\n",
        "\n",
        "    letter_offset += bound_dims\n",
        "    for i in range(output_dims):\n",
        "      char = _CHR_IDX[i + letter_offset]\n",
        "      kernel_str += char\n",
        "      output_str += char\n",
        "\n",
        "    return input_str + \",\" + kernel_str + \"->\" + output_str\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    input_shape = tf.TensorShape(input_shape)\n",
        "    input_rank = input_shape.rank\n",
        "    free_input_dims = input_rank - self._num_summed_dimensions\n",
        "    output_dims = len(self._output_shape)\n",
        "\n",
        "    self._einsum_string = self._build_einsum_string(free_input_dims,\n",
        "                                                    self._num_summed_dimensions,\n",
        "                                                    output_dims)\n",
        "\n",
        "    # This is only saved for testing purposes.\n",
        "    self._kernel_shape = (\n",
        "        input_shape[free_input_dims:].concatenate(self._output_shape))\n",
        "\n",
        "    self._kernel = self.add_weight(\n",
        "        \"kernel\",\n",
        "        shape=self._kernel_shape,\n",
        "        initializer=self._kernel_initializer,\n",
        "        regularizer=self._kernel_regularizer,\n",
        "        constraint=self._kernel_constraint,\n",
        "        dtype=self.dtype,\n",
        "        trainable=True)\n",
        "    if self._use_bias:\n",
        "      self._bias = self.add_weight(\n",
        "          \"bias\",\n",
        "          shape=self._output_shape,\n",
        "          initializer=self._bias_initializer,\n",
        "          regularizer=self._bias_regularizer,\n",
        "          constraint=self._bias_constraint,\n",
        "          dtype=self.dtype,\n",
        "          trainable=True)\n",
        "    else:\n",
        "      self._bias = None\n",
        "    super(DenseEinsum, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        \"output_shape\":\n",
        "            self._output_shape,\n",
        "        \"num_summed_dimensions\":\n",
        "            self._num_summed_dimensions,\n",
        "        \"activation\":\n",
        "            tf.keras.activations.serialize(self._activation),\n",
        "        \"use_bias\":\n",
        "            self._use_bias,\n",
        "        \"kernel_initializer\":\n",
        "            tf.keras.initializers.serialize(self._kernel_initializer),\n",
        "        \"bias_initializer\":\n",
        "            tf.keras.initializers.serialize(self._bias_initializer),\n",
        "        \"kernel_regularizer\":\n",
        "            tf.keras.regularizers.serialize(self._kernel_regularizer),\n",
        "        \"bias_regularizer\":\n",
        "            tf.keras.regularizers.serialize(self._bias_regularizer),\n",
        "        \"activity_regularizer\":\n",
        "            tf.keras.regularizers.serialize(self._activity_regularizer),\n",
        "        \"kernel_constraint\":\n",
        "            tf.keras.constraints.serialize(self._kernel_constraint),\n",
        "        \"bias_constraint\":\n",
        "            tf.keras.constraints.serialize(self._bias_constraint)\n",
        "    }\n",
        "    base_config = super(DenseEinsum, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    ret = tf.einsum(self._einsum_string, inputs, self._kernel)\n",
        "    if self._use_bias:\n",
        "      ret += self._bias\n",
        "    if self._activation is not None:\n",
        "      ret = self._activation(ret)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox1S0_treoKk"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2022 The Google Research Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Implementation of multiheaded FAVOR-attention & FAVOR-self-attention layers.\n",
        "Prefix Sum Tensorflow implementation by Valerii Likhosherstov.\n",
        "\"\"\"\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "BIG_CONSTANT = 1e8\n",
        "\n",
        "def create_products_of_givens_rotations(dim, seed):\n",
        "  r\"\"\"Constructs a 2D-tensor which is a product of Givens random rotations.\n",
        "  Constructs a 2D-tensor of the form G_1 * ... * G_k, where G_i is a Givens\n",
        "  random rotation. The resulting tensor mimics a matrix taken uniformly at\n",
        "  random form the orthogonal group.\n",
        "  Args:\n",
        "    dim: number of rows/columns of the resulting 2D-tensor.\n",
        "    seed: random seed.\n",
        "  Returns:\n",
        "    The product of Givens random rotations.\n",
        "  \"\"\"\n",
        "  nb_givens_rotations = dim * int(math.ceil(math.log(float(dim))))\n",
        "  q = np.eye(dim, dim)\n",
        "  np.random.seed(seed)\n",
        "  for _ in range(nb_givens_rotations):\n",
        "    random_angle = math.pi * np.random.uniform()\n",
        "    random_indices = np.random.choice(dim, 2)\n",
        "    index_i = min(random_indices[0], random_indices[1])\n",
        "    index_j = max(random_indices[0], random_indices[1])\n",
        "    slice_i = q[index_i]\n",
        "    slice_j = q[index_j]\n",
        "    new_slice_i = math.cos(random_angle) * slice_i + math.sin(\n",
        "        random_angle) * slice_j\n",
        "    new_slice_j = -math.sin(random_angle) * slice_i + math.cos(\n",
        "        random_angle) * slice_j\n",
        "    q[index_i] = new_slice_i\n",
        "    q[index_j] = new_slice_j\n",
        "  return tf.cast(tf.constant(q), dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "def create_projection_matrix(m, d, seed=0, scaling=0, struct_mode=False):\n",
        "  r\"\"\"Constructs the matrix of random projections.\n",
        "  Constructs a matrix of random orthogonal projections. Each projection vector\n",
        "  has direction chosen uniformly at random and either deterministic length\n",
        "  \\sqrt{d} or length taken from the \\chi(d) distribution (in the latter case\n",
        "  marginal distributions of the projections are d-dimensional Gaussian vectors\n",
        "  with associated identity covariance matrix).\n",
        "  Args:\n",
        "    m: number of random projections.\n",
        "    d: dimensionality of each random projection.\n",
        "    seed: random seed used to construct projections.\n",
        "    scaling: 1 if all the random projections need to be renormalized to have\n",
        "      length \\sqrt{d}, 0 if the lengths of random projections should follow\n",
        "      \\chi(d) distribution.\n",
        "    struct_mode: if True then products of Givens rotations will be used to\n",
        "      construct random orthogonal matrix. This bypasses Gram-Schmidt\n",
        "      orthogonalization.\n",
        "  Returns:\n",
        "    The matrix of random projections of the shape [m, d].\n",
        "  \"\"\"\n",
        "  print('m',m,'d',d)\n",
        "  nb_full_blocks = int(m / d)\n",
        "  block_list = []\n",
        "  current_seed = seed\n",
        "  for _ in range(nb_full_blocks):\n",
        "    if struct_mode:\n",
        "      q = create_products_of_givens_rotations(d, seed)\n",
        "    else:\n",
        "      unstructured_block = tf.random.normal((d, d), seed=current_seed)\n",
        "      q, _ = tf.linalg.qr(unstructured_block)\n",
        "      q = tf.transpose(q)\n",
        "    block_list.append(q)\n",
        "    current_seed += 1\n",
        "  remaining_rows = m - nb_full_blocks * d\n",
        "  if remaining_rows > 0:\n",
        "    if struct_mode:\n",
        "      q = create_products_of_givens_rotations(d, seed)\n",
        "    else:\n",
        "      unstructured_block = tf.random.normal((d, d), seed=current_seed)\n",
        "      q, _ = tf.linalg.qr(unstructured_block)\n",
        "      q = tf.transpose(q)\n",
        "    block_list.append(q[0:remaining_rows])\n",
        "  final_matrix = tf.experimental.numpy.vstack(block_list)\n",
        "  current_seed += 1\n",
        "\n",
        "  if scaling == 0:\n",
        "    multiplier = tf.norm(tf.random.normal((m, d), seed=current_seed), axis=1)\n",
        "  elif scaling == 1:\n",
        "    multiplier = tf.math.sqrt(float(d)) * tf.ones((m))\n",
        "  else:\n",
        "    raise ValueError(\"Scaling must be one of {0, 1}. Was %s\" % scaling)\n",
        "\n",
        "  return tf.linalg.matmul(tf.linalg.diag(multiplier), final_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psd6GGsFeoKl"
      },
      "outputs": [],
      "source": [
        "#This is number 1 of the kernal transformation options, we can add the other options pretty easily I think \n",
        "\n",
        "def relu_kernel_transformation(data,\n",
        "                               is_query,\n",
        "                               projection_matrix=None,\n",
        "                               numerical_stabilizer=0.001):\n",
        "  del is_query\n",
        "  if projection_matrix is None:\n",
        "    return tf.nn.relu(data) + numerical_stabilizer\n",
        "  else:\n",
        "    ratio = 1.0 / tf.math.sqrt(\n",
        "        tf.dtypes.cast(projection_matrix.shape[0], tf.float32))\n",
        "    data_dash = ratio * tf.einsum(\"blhd,md->blhm\", data, projection_matrix)\n",
        "    return tf.nn.relu(data_dash) + numerical_stabilizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcesGJ5awrFq"
      },
      "outputs": [],
      "source": [
        "def softmax_kernel_transformation(data,\n",
        "                                  is_query,\n",
        "                                  projection_matrix=None,\n",
        "                                  numerical_stabilizer=0.000001):\n",
        "  \"\"\"Computes random features for the softmax kernel using FAVOR+ mechanism.\n",
        "  Computes random features for the softmax kernel using FAVOR+ mechanism from\n",
        "  https://arxiv.org/pdf/2009.14794.pdf.\n",
        "  Args:\n",
        "    data: input data tensor of the shape [B, L, H, D], where: B - batch\n",
        "      dimension, L - attention dimensions, H - heads, D - features.\n",
        "    is_query: indicates whether input data is a query oor key tensor.\n",
        "    projection_matrix: random Gaussian matrix of shape [M, D], where M stands\n",
        "      for the number of random features and each D x D sub-block has pairwise\n",
        "      orthogonal rows.\n",
        "    numerical_stabilizer: small positive constant for numerical stability.\n",
        "  Returns:\n",
        "    Corresponding kernel feature map.\n",
        "  \"\"\"\n",
        "  print('proj mat',projection_matrix)\n",
        "  print(\"insoftmax\",data.shape)\n",
        "  data_normalizer = 1.0 / (\n",
        "      tf.math.sqrt(tf.math.sqrt(tf.dtypes.cast(data.shape[-1], tf.float32))))\n",
        "  print(\"calcs1\")\n",
        "  data = data_normalizer * data\n",
        "  ratio = 1.0 / tf.math.sqrt(\n",
        "      tf.dtypes.cast(projection_matrix.shape[0], tf.float32))\n",
        "  print(\"calcs2\")\n",
        "  data_dash = tf.einsum(\"blhd,md->blhm\", data, projection_matrix)\n",
        "  print(\"calcs3\")\n",
        "  diag_data = tf.math.square(data)\n",
        "  print(\"calcs4\")\n",
        "  diag_data = tf.math.reduce_sum(\n",
        "      diag_data, axis=tf.keras.backend.ndim(data) - 1)\n",
        "  print(\"calcs5\")\n",
        "  diag_data = diag_data / 2.0\n",
        "  diag_data = tf.expand_dims(diag_data, axis=tf.keras.backend.ndim(data) - 1)\n",
        "  last_dims_t = (len(data_dash.shape) - 1,)\n",
        "  attention_dims_t = (len(data_dash.shape) - 3,)\n",
        "  print(\"calcs\")\n",
        "  if is_query:\n",
        "    data_dash = ratio * (\n",
        "        tf.math.exp(data_dash - diag_data - tf.math.reduce_max(\n",
        "            data_dash, axis=last_dims_t, keepdims=True)) + numerical_stabilizer)\n",
        "  else:\n",
        "    data_dash = ratio * (\n",
        "        tf.math.exp(data_dash - diag_data - tf.math.reduce_max(\n",
        "            data_dash, axis=last_dims_t + attention_dims_t, keepdims=True)) +\n",
        "        numerical_stabilizer)\n",
        "\n",
        "  return data_dash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsDuCSsweoKl"
      },
      "outputs": [],
      "source": [
        "#numerator and denominator calculations \n",
        "def noncausal_numerator(qs, ks, vs):\n",
        "  \"\"\"Computes not-normalized FAVOR noncausal attention AV.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "    vs: value tensor of the shape [L,B,H,D].\n",
        "  Returns:\n",
        "    Not-normalized FAVOR noncausal attention AV.\n",
        "  \"\"\"\n",
        "  kvs = tf.einsum(\"lbhm,lbhd->bhmd\", ks, vs)\n",
        "  return tf.einsum(\"lbhm,bhmd->lbhd\", qs, kvs)\n",
        "\n",
        "\n",
        "def noncausal_denominator(qs, ks):\n",
        "  \"\"\"Computes FAVOR normalizer in noncausal attention.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "  Returns:\n",
        "    FAVOR normalizer in noncausal attention.\n",
        "  \"\"\"\n",
        "  all_ones = tf.ones([ks.shape[0]])\n",
        "  ks_sum = tf.einsum(\"lbhm,l->bhm\", ks, all_ones)\n",
        "  return tf.einsum(\"lbhm,bhm->lbh\", qs, ks_sum)\n",
        "\n",
        "\n",
        "@tf.custom_gradient\n",
        "def causal_numerator(qs, ks, vs):\n",
        "  \"\"\"Computes not-normalized FAVOR causal attention A_{masked}V.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "    vs: value tensor of the shape [L,B,H,D].\n",
        "  Returns:\n",
        "    Not-normalized FAVOR causal attention A_{masked}V.\n",
        "  \"\"\"\n",
        "\n",
        "  result = []\n",
        "  sums = tf.zeros_like(tf.einsum(\"ijk,ijl->ijkl\", ks[0], vs[0]))\n",
        "\n",
        "  for index in range(qs.shape[0]):\n",
        "    sums = sums + tf.einsum(\"ijk,ijl->ijkl\", ks[index], vs[index])\n",
        "    result.append(tf.einsum(\"ijkl,ijk->ijl\", sums, qs[index])[None, Ellipsis])\n",
        "\n",
        "  result = tf.concat(result, axis=0)\n",
        "\n",
        "  def grad(res_grad):\n",
        "\n",
        "    grads = tf.zeros_like(tf.einsum(\"ijk,ijl->ijkl\", ks[0], vs[0]))\n",
        "\n",
        "    gr_sums = sums\n",
        "\n",
        "    q_grads = []\n",
        "    k_grads = []\n",
        "    v_grads = []\n",
        "\n",
        "    for index in range(qs.shape[0] - 1, -1, -1):\n",
        "\n",
        "      q_grads.append(\n",
        "          tf.einsum(\"ijkl,ijl->ijk\", gr_sums, res_grad[index])[None, Ellipsis])\n",
        "      grads = grads + tf.einsum(\"ijk,ijl->ijkl\", qs[index], res_grad[index])\n",
        "      k_grads.append(tf.einsum(\"ijkl,ijl->ijk\", grads, vs[index])[None, Ellipsis])\n",
        "      v_grads.append(tf.einsum(\"ijkl,ijk->ijl\", grads, ks[index])[None, Ellipsis])\n",
        "      gr_sums = gr_sums - tf.einsum(\"ijk,ijl->ijkl\", ks[index], vs[index])\n",
        "\n",
        "    q_grads = tf.concat(q_grads[::-1], axis=0)\n",
        "    k_grads = tf.concat(k_grads[::-1], axis=0)\n",
        "    v_grads = tf.concat(v_grads[::-1], axis=0)\n",
        "\n",
        "    return q_grads, k_grads, v_grads\n",
        "\n",
        "  return result, grad\n",
        "\n",
        "\n",
        "@tf.custom_gradient\n",
        "def causal_denominator(qs, ks):\n",
        "  \"\"\"Computes FAVOR normalizer in causal attention.\n",
        "  Args:\n",
        "    qs: query_prime tensor of the shape [L,B,H,M].\n",
        "    ks: key_prime tensor of the shape [L,B,H,M].\n",
        "  Returns:\n",
        "    FAVOR normalizer in causal attention.\n",
        "  \"\"\"\n",
        "\n",
        "  result = []\n",
        "  sums = tf.zeros_like(ks[0])\n",
        "\n",
        "  for index in range(qs.shape[0]):\n",
        "    sums = sums + ks[index]\n",
        "    result.append(tf.reduce_sum(qs[index] * sums, axis=2)[None, Ellipsis])\n",
        "\n",
        "  result = tf.concat(result, axis=0)\n",
        "\n",
        "  def grad(res_grad):\n",
        "\n",
        "    k_grad = tf.zeros_like(ks[0])\n",
        "\n",
        "    gr_sums = sums\n",
        "\n",
        "    q_grads = []\n",
        "    k_grads = []\n",
        "\n",
        "    for index in range(qs.shape[0] - 1, -1, -1):\n",
        "\n",
        "      q_grads.append(\n",
        "          tf.einsum(\"ijk,ij->ijk\", gr_sums, res_grad[index])[None, Ellipsis])\n",
        "      k_grad = k_grad + tf.einsum(\"ijk,ij->ijk\", qs[index], res_grad[index])\n",
        "      k_grads.append(k_grad[None, Ellipsis])\n",
        "      gr_sums = gr_sums - ks[index]\n",
        "\n",
        "    q_grads = tf.concat(q_grads[::-1], axis=0)\n",
        "    k_grads = tf.concat(k_grads[::-1], axis=0)\n",
        "\n",
        "    return q_grads, k_grads\n",
        "\n",
        "  return result, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC1anNfGeoKl"
      },
      "outputs": [],
      "source": [
        "#this is the new version of the attention code from the transformer version (this is the main calculation of the performer attention class) \n",
        "def favor_attention(query,\n",
        "                    key,\n",
        "                    value,\n",
        "                    kernel_transformation,\n",
        "                    causal,\n",
        "                    projection_matrix=None):\n",
        "  \"\"\"Computes FAVOR normalized attention.\n",
        "  #push again \n",
        "  Args:\n",
        "    query: query tensor.\n",
        "    key: key tensor.\n",
        "    value: value tensor.\n",
        "    kernel_transformation: transformation used to get finite kernel features.\n",
        "    causal: whether attention is causal or not.\n",
        "    projection_matrix: projection matrix to be used.\n",
        "  Returns:\n",
        "    FAVOR normalized attention.\n",
        "  \"\"\"\n",
        "  print('k',kernel_transformation)\n",
        "  query_prime = kernel_transformation(query, True,\n",
        "                                      projection_matrix)  # [B,L,H,M]\n",
        "  print('not failed')\n",
        "  key_prime = kernel_transformation(key, False, projection_matrix)  # [B,L,H,M]\n",
        "  query_prime = tf.transpose(query_prime, [1, 0, 2, 3])  # [L,B,H,M]\n",
        "  key_prime = tf.transpose(key_prime, [1, 0, 2, 3])  # [L,B,H,M]\n",
        "  value = tf.transpose(value, [1, 0, 2, 3])  # [L,B,H,D]\n",
        "\n",
        "  if causal:\n",
        "    av_attention = causal_numerator(query_prime, key_prime, value)\n",
        "    attention_normalizer = causal_denominator(query_prime, key_prime)\n",
        "  else:\n",
        "    av_attention = noncausal_numerator(query_prime, key_prime, value)\n",
        "    attention_normalizer = noncausal_denominator(query_prime, key_prime)\n",
        "  # TODO(kchoro): Add more comments.\n",
        "  av_attention = tf.transpose(av_attention, [1, 0, 2, 3])\n",
        "  attention_normalizer = tf.transpose(attention_normalizer, [1, 0, 2])\n",
        "  attention_normalizer = tf.expand_dims(attention_normalizer,\n",
        "                                        len(attention_normalizer.shape))\n",
        "  return av_attention / attention_normalizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW27t0A_eoKl"
      },
      "outputs": [],
      "source": [
        "#new attention class (starting with relu_kernel_transformation)\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "  \"\"\"Multi-headed attention layer.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               hidden_size,\n",
        "               num_heads,\n",
        "               dropout_rate,\n",
        "               kernel_transformation=softmax_kernel_transformation,\n",
        "               numerical_stabilizer=0.001,\n",
        "               causal=False,\n",
        "               projection_matrix_type = True,\n",
        "               nb_random_features=13):\n",
        "    \"\"\"Initialize Attention.\n",
        "    Args:\n",
        "      hidden_size: int, output dim of hidden layer.\n",
        "      num_heads: int, number of heads to repeat the same attention structure.\n",
        "      dropout_rate: float, dropout rate inside attention for training.\n",
        "      kernel_transformation: transformation used to produce kernel features for\n",
        "        attention.\n",
        "      numerical_stabilizer: used to bound away from zero kernel values.\n",
        "      causal: whether attention is causal or not.\n",
        "      projection_matrix_type: None if Identity should be used, otherwise random\n",
        "        projection matrix will be applied.\n",
        "      nb_random_features: number of random features to be used (relevant only if\n",
        "        projection_matrix is not None).\n",
        "    \"\"\"\n",
        "    if hidden_size % num_heads:\n",
        "      raise ValueError(\n",
        "          \"Hidden size ({}) must be divisible by the number of heads ({}).\"\n",
        "          .format(hidden_size, num_heads))\n",
        "\n",
        "    super(Attention, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.kernel_transformation = kernel_transformation\n",
        "    self.numerical_stabilizer = numerical_stabilizer\n",
        "    self.causal = causal\n",
        "    self.projection_matrix_type = projection_matrix_type\n",
        "    self.nb_random_features = nb_random_features\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \"\"\"Builds the layer.\"\"\"\n",
        "    # Layers for linearly projecting the queries, keys, and values.\n",
        "    size_per_head = self.hidden_size // self.num_heads\n",
        "\n",
        "    def _glorot_initializer(fan_in, fan_out):\n",
        "      limit = math.sqrt(6.0 / (fan_in + fan_out))\n",
        "      return tf.keras.initializers.RandomUniform(minval=-limit, maxval=limit)\n",
        "\n",
        "    attention_initializer = _glorot_initializer(input_shape.as_list()[-1],\n",
        "                                                self.hidden_size)\n",
        "    self.query_dense_layer = DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"query\")\n",
        "    self.key_dense_layer = DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"key\")\n",
        "    self.value_dense_layer = DenseEinsum(\n",
        "        output_shape=(self.num_heads, size_per_head),\n",
        "        kernel_initializer=attention_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"value\")\n",
        "\n",
        "    output_initializer = _glorot_initializer(self.hidden_size, self.hidden_size)\n",
        "    self.output_dense_layer = DenseEinsum(\n",
        "        output_shape=self.hidden_size,\n",
        "        num_summed_dimensions=2,\n",
        "        kernel_initializer=output_initializer,\n",
        "        use_bias=False,\n",
        "        name=\"output_transform\")\n",
        "    super(Attention, self).build(input_shape)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        \"hidden_size\": self.hidden_size,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dropout_rate\": self.dropout_rate,\n",
        "    }\n",
        "\n",
        "  def call(self,\n",
        "           query_input,\n",
        "           source_input,\n",
        "           training,\n",
        "           cache=None,\n",
        "           decode_loop_step=None):\n",
        "    \"\"\"Apply attention mechanism to query_input and source_input.\n",
        "    Args:\n",
        "      query_input: A tensor with shape [batch_size, length_query, hidden_size].\n",
        "      source_input: A tensor with shape [batch_size, length_source,\n",
        "        hidden_size].\n",
        "      bias: A tensor with shape [batch_size, 1, length_query, length_source],\n",
        "        the attention bias that will be added to the result of the dot product.\n",
        "      training: A bool, whether in training mode or not.\n",
        "      cache: (Used during prediction) A dictionary with tensors containing\n",
        "        results of previous attentions. The dictionary must have the items:\n",
        "            {\"k\": tensor with shape [batch_size, i, heads, dim_per_head],\n",
        "             \"v\": tensor with shape [batch_size, i, heads, dim_per_head]} where\n",
        "               i is the current decoded length for non-padded decode, or max\n",
        "               sequence length for padded decode.\n",
        "      decode_loop_step: An integer, step number of the decoding loop. Used only\n",
        "        for autoregressive inference on TPU.\n",
        "    Returns:\n",
        "      Attention layer output with shape [batch_size, length_query, hidden_size]\n",
        "    \"\"\"\n",
        "    # Linearly project the query, key and value using different learned\n",
        "    # projections. Splitting heads is automatically done during the linear\n",
        "    # projections --> [batch_size, length, num_heads, dim_per_head].\n",
        "    query = self.query_dense_layer(query_input)\n",
        "    key = self.key_dense_layer(source_input)\n",
        "    value = self.value_dense_layer(source_input)\n",
        "\n",
        "    print('calling')\n",
        "\n",
        "    dim = query.shape[-1]\n",
        "    print('proj2')\n",
        "    seed = tf.math.ceil(tf.math.abs(tf.math.reduce_sum(query) * BIG_CONSTANT))\n",
        "    print('proj3')\n",
        "    seed = tf.dtypes.cast(seed, tf.int32)\n",
        "    print('proj4')\n",
        "    projection_matrix = create_projection_matrix(self.nb_random_features, dim, seed=seed)\n",
        "    print('proj5')\n",
        "\n",
        "    if cache is not None:\n",
        "      print('should not be here')\n",
        "      # Combine cached keys and values with new keys and values.\n",
        "      if decode_loop_step is not None:\n",
        "        cache_k_shape = cache[\"k\"].shape.as_list()\n",
        "        indices = tf.reshape(\n",
        "            tf.one_hot(decode_loop_step, cache_k_shape[1], dtype=key.dtype),\n",
        "            [1, cache_k_shape[1], 1, 1])\n",
        "        key = cache[\"k\"] + key * indices\n",
        "        cache_v_shape = cache[\"v\"].shape.as_list()\n",
        "        indices = tf.reshape(\n",
        "            tf.one_hot(decode_loop_step, cache_v_shape[1], dtype=value.dtype),\n",
        "            [1, cache_v_shape[1], 1, 1])\n",
        "        value = cache[\"v\"] + value * indices\n",
        "      else:\n",
        "        key = tf.concat([tf.cast(cache[\"k\"], key.dtype), key], axis=1)\n",
        "        value = tf.concat([tf.cast(cache[\"v\"], value.dtype), value], axis=1)\n",
        "\n",
        "      # Update cache\n",
        "      #cache[\"k\"] = key\n",
        "      #cache[\"v\"] = value\n",
        "\n",
        "    print('att output')\n",
        "    attention_output = favor_attention(query, key, value,\n",
        "                                       self.kernel_transformation, self.causal,\n",
        "                                       projection_matrix)\n",
        "    attention_output = self.output_dense_layer(attention_output)\n",
        "    return attention_output\n",
        "\n",
        "\n",
        "class SelfAttention(Attention):\n",
        "  \"\"\"Multiheaded self-attention layer.\"\"\"\n",
        "\n",
        "  def call(self,\n",
        "           query_input,\n",
        "           training,\n",
        "           cache=None,\n",
        "           decode_loop_step=None):\n",
        "    return super(SelfAttention, self).call(query_input, query_input,training, cache, decode_loop_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRlFJ1xynH5h"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqAOHEePeoKm"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    #IMPORTANT CHANGE: obv we added code that is different from the og attention model above, but this is \n",
        "    #the main change within the framework from the original transformer_v2 model, we just call the new \n",
        "    #self attention class instead of the traditional self attention \n",
        "    self.self_attention = SelfAttention(\n",
        "        hidden_size=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout_rate=dropout_rate)\n",
        "        #kernel_transformation = softmax_kernel_transformation\n",
        "    \n",
        "    self.causal_self_attention = SelfAttention(\n",
        "        causal = True,\n",
        "        hidden_size=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout_rate=dropout_rate)\n",
        "        #kernel_transformation = softmax_kernel_transformation    \n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    #bias = tf.ones([x.shape[0], 1, 13, 13])\n",
        "    print('eh',x.shape)\n",
        "    x = self.self_attention(x,training=True)\n",
        "    print('hh',x.shape)\n",
        "    x = self.causal_self_attention(x,training=True)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "261ko-x0eoKm",
        "outputId": "a53a65f7-dbe1-43be-d748-c06926083ecd"
      },
      "outputs": [],
      "source": [
        "x1_df_test = x1_df[0:336]\n",
        "x1_df_test_time = x1_df_test[:,-1]\n",
        "x1_test_data = df[STREETS][0:336].to_numpy()\n",
        "print()\n",
        "positional_em = PositionalEmbedding(x1_df_test_time,0.4)\n",
        "x1_test, y1_test = positional_em(x1_test_data)\n",
        "print(x1_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDSC8tEreoKm",
        "outputId": "2ca0a4ef-5e41-469c-e41f-5dbe8efef4be"
      },
      "outputs": [],
      "source": [
        "sample_encoder_layer = EncoderLayer(d_model=keys_dim, num_heads=9, dff=keys_dim)\n",
        "encoder_test = positional_em(x1_test_data)[0]\n",
        "print(encoder_test.shape)\n",
        "print(sample_encoder_layer(encoder_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLOxGvXyeoKm"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(\n",
        "                    d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff\n",
        "                     )\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    print('try embedd',x.shape)\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      print()\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlLO67jOeoKm",
        "outputId": "a2a6cd13-0602-42e4-a678-39a1a48f3dc4"
      },
      "outputs": [],
      "source": [
        "sample_encoder = Encoder(num_layers=2,\n",
        "                         d_model=keys_dim,\n",
        "                         num_heads=3,\n",
        "                         dff=keys_dim\n",
        "                         )\n",
        "\n",
        "sample_encoder_output = sample_encoder(encoder_test, training=False)\n",
        "\n",
        "# Print the shape.\n",
        "print(encoder_test.shape)\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY5N-ebf6pBe"
      },
      "outputs": [],
      "source": [
        "#encoder only transformer \n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           dropout_rate=dropout_rate)\n",
        "    #flattening before the final layer \n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.final_layer = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    print('input_shape',inputs.shape)\n",
        "    x = inputs\n",
        "    x = self.encoder(x)  # (batch_size, target_len, d_model)\n",
        "    \n",
        "\n",
        "    # Final linear layer output.\n",
        "    x = self.flatten(x)\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    \n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfDPZTNreoKm"
      },
      "outputs": [],
      "source": [
        "num_layers = 2\n",
        "d_model = keys_dim\n",
        "dff = keys_dim\n",
        "num_heads = 3\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfxrE5uYeoKm"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    dropout_rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0K8Y1LReoKm",
        "outputId": "d22d7dd2-e84b-43d0-a734-088bb8c99bd0"
      },
      "outputs": [],
      "source": [
        "print(x1_df.shape)\n",
        "length = 2832\n",
        "#80, 10, 10 split from train val test \n",
        "val_step = int(0.1 * length) * 2\n",
        "test_step = int(0.1 * length)\n",
        "\n",
        "val_s = length - val_step\n",
        "test_s = length - test_step\n",
        "\n",
        "x_train = x1_df[:val_s]\n",
        "x_val = x1_df[val_s:test_s]\n",
        "x_test= x1_df[test_s:]\n",
        "\n",
        "\n",
        "\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2jF2kcjeoKm",
        "outputId": "094c9b4d-5545-4939-d177-b5e3928685f8"
      },
      "outputs": [],
      "source": [
        "#positionally encode and divide all data sets into batches \n",
        "train_time = x_train[:,-1]\n",
        "train_feat = x_train[:,:-1]\n",
        "val_time = x_val[:,-1]\n",
        "val_feat = x_val[:,:-1]\n",
        "test_time = x_test[:,-1]\n",
        "test_feat = x_test[:,:-1]\n",
        "print(x_test.shape)\n",
        "print(x_val.shape)\n",
        "print(test_time.shape)\n",
        "\n",
        "train_pos_enc = PositionalEmbedding(train_time,0.2)\n",
        "val_pos_enc = PositionalEmbedding(val_time,0.2)\n",
        "test_pos_enc = PositionalEmbedding(test_time,0.2)\n",
        "\n",
        "\n",
        "x_train,y_train = train_pos_enc(np.asarray(train_feat).astype('float32'))\n",
        "x_val,y_val = val_pos_enc(np.asarray(val_feat).astype('float32'))\n",
        "x_test,y_test = test_pos_enc(np.asarray(test_feat).astype('float32'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6M41g0feoKm",
        "outputId": "7bfb47b4-8de9-4922-cac8-9664e4d82940"
      },
      "outputs": [],
      "source": [
        "tf.shape(x_test).numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "2KrlU5syeoKn",
        "outputId": "dfacde93-464d-4d79-86b7-b8b21cc5a9ec"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_train[0,:,2])\n",
        "\n",
        "plt.scatter(12,y_train[0,:,2])\n",
        "print(y_train[0,:,2])\n",
        "print(x_train[0,:,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QclydFsweoKn",
        "outputId": "0d324906-d8db-4826-f46a-c48423e902d8"
      },
      "outputs": [],
      "source": [
        "train_dataloader = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(64)\n",
        "val_dataloader = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(10000).batch(64)\n",
        "test_dataloader = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
        "\n",
        "train_batch_size = 2253\n",
        "val_batch_size = 270\n",
        "test_batch_size = 270\n",
        "\n",
        "print(train_dataloader)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WCueYWGeoKn"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model, log=False):\n",
        "    for traffic_batch, labels_batch in dataloader:\n",
        "        traffic = traffic_batch\n",
        "        print(traffic.shape)\n",
        "        predict_flow = model(traffic)\n",
        "        mse = tf.keras.losses.MeanSquaredError()\n",
        "        mse = mse(labels_batch,predict_flow ).numpy()\n",
        "    \n",
        "    return predict_flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WiTRTQaeoKn",
        "outputId": "21c4e2d6-9fcc-4fd4-8146-76436c5f98e0"
      },
      "outputs": [],
      "source": [
        "evaluate(test_dataloader,transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDMEHkdWeoKn"
      },
      "outputs": [],
      "source": [
        "def loss(y_true, y_pred):\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    y = mse(y_true, y_pred)\n",
        "    return y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tngOZr8reoKn"
      },
      "outputs": [],
      "source": [
        "transformer.compile(optimizer=optimizer, \n",
        "                  loss = tf.keras.losses.MeanSquaredError(), \n",
        "                  metrics= [tf.keras.losses.MeanSquaredError()]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3KB5CXjeoKn",
        "outputId": "8a0553c0-e357-492e-9b7f-ef4fba42247a"
      },
      "outputs": [],
      "source": [
        "transformer.fit(train_dataloader,epochs = 7,validation_data = val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcd8i_7JwpLP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5ctLDoweoKn"
      },
      "outputs": [],
      "source": [
        "response = evaluate(test_dataloader,transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd24qZ9VeoKn"
      },
      "outputs": [],
      "source": [
        "res = transformer.predict(x_test)\n",
        "res[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq-TBPPzeoKn"
      },
      "outputs": [],
      "source": [
        "res2 = tf.reduce_mean(res,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgWRZuD-eoKn"
      },
      "outputs": [],
      "source": [
        "plt.plot(res2)\n",
        "#plt.plot(tf.reduce_mean(y_test,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1ys0BTneoKn"
      },
      "outputs": [],
      "source": [
        "print(max(res2))\n",
        "print(min(res2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "39282081ff6844baad5334981d710ee8987f21358ca5c4d02825da922a75398f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
