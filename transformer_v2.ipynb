{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70yJX_h9yA-F"
      },
      "outputs": [],
      "source": [
        "#setup \n",
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "!pip install scikit-learn\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "!pip install geopandas\n",
        "!pip install geojson\n",
        "!pip install shapely\n",
        "import geopandas as gpd\n",
        "import geojson\n",
        "import geopandas as gpd\n",
        "from fiona.crs import from_epsg\n",
        "import os, json\n",
        "from shapely.geometry import shape, Point, Polygon, MultiPoint\n",
        "from geopandas.tools import sjoin\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import seaborn as sns; sns.set()\n",
        "from IPython.display import Image\n",
        "#from branca.colormap import  linear\n",
        "import json\n",
        "#import branca.colormap as cm\n",
        "!pip install folium\n",
        "import folium\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (4, 3)\n",
        "mpl.rcParams['axes.grid'] = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnPnRoIYyeOy"
      },
      "outputs": [],
      "source": [
        "new_table = pd.read_csv('https://raw.githubusercontent.com/giobbu/App_Traff_Forecast_DeapLearn/master/data/Flow_BEL_street_30min.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_belgium = gpd.read_file('https://raw.githubusercontent.com/giobbu/App_Traff_Forecast_DeapLearn/master/data/Belgium_streets.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0JEA8J30c5D",
        "outputId": "590d1748-c677-4247-ad20-34da57402ee5"
      },
      "outputs": [],
      "source": [
        "# this source: \n",
        "# the https://www.kaggle.com/code/giobbu/seasonal-persistence-model/notebook#Results-Comparison-between-Seasonal-model-(baseline)-and-LSTM-encoder-decoder-model\n",
        "# selects the roads that have an average traffic flow of 10 or larger, we also do this, this code is from the notebook listed above \n",
        "\n",
        "table_index = new_table.iloc[:,1:]\n",
        "ALL_STREETS = list(table_index.columns.values)\n",
        "\n",
        "mean_flow =[]\n",
        "new_street=[]\n",
        "\n",
        "mean_value = 20\n",
        "\n",
        "for street in ALL_STREETS:\n",
        "    \n",
        "    single_street=table_index[street]\n",
        "    mean = np.mean(single_street)\n",
        "    mean_flow.append(mean)\n",
        "    new_street.append(street)\n",
        "    \n",
        "    \n",
        "df_mean_flow = pd.DataFrame({'street_index':new_street, 'mean_flow': mean_flow})\n",
        "print('')\n",
        "print(df_mean_flow.head())\n",
        "print('')\n",
        "\n",
        "STREETS = df_mean_flow[(df_mean_flow['mean_flow'] >= mean_value)] \n",
        "STREETS = STREETS.sort_values(by=['street_index'])\n",
        "STREETS = list(STREETS.street_index)\n",
        "\n",
        "keys_dim = 1683\n",
        "keys_dim_time = 1686\n",
        "\n",
        "print('considering a average traffic flow of ' + str(mean_value)+' per street')\n",
        "print('')\n",
        "print('mean traffic flow '+str(mean_value)+ ' ---> number of street segments: ' + str(len(STREETS)))\n",
        "print(len(STREETS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#this is for selecting street indecies for extension \n",
        "def in_area(poly):\n",
        "  bounds = poly.bounds\n",
        "  lon_lat_list = [[ 4.14878,50.35045], [4.14878,50.35045], [4.54878,50.35045 ], [4.54878,51.05045], [4.14878,51.05045]]\n",
        "  min_x = lon_lat_list[0][0]\n",
        "  max_x = lon_lat_list[2][0]\n",
        "  min_y = lon_lat_list[0][1]\n",
        "  max_y = lon_lat_list[3][1]\n",
        "\n",
        "  if (((bounds[0]<= max_x and bounds[0] >=min_x) or (bounds[2]<= max_x and bounds[2] >=min_x))and\n",
        "      ((bounds[1]<= max_y and bounds[1] >=min_y) or (bounds[3]<= max_y and bounds[3] >=min_y))):  \n",
        "      return True \n",
        "  return False \n",
        "\n",
        "streets_in_area = []\n",
        "df_belgium['is_in_area']  = df_belgium.apply(lambda x: in_area(x['geometry']),axis = 1)\n",
        "the_streets = df_belgium[df_belgium['is_in_area'] ==True] \n",
        "\n",
        "STREETS2 = the_streets.index.tolist()\n",
        "len(STREETS2)\n",
        "STREETS2 = list(map(str, STREETS2))\n",
        "STREETS2 = [x + '.0' for x in STREETS2]\n",
        "#STREETS = list(set(STREETS) & set(STREETS2))\n",
        "print(len(STREETS))\n",
        "#keys_dim = 1042\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8PW_uZbWlBB"
      },
      "outputs": [],
      "source": [
        "new_table['datetime'] = pd.to_datetime(new_table['datetime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "ZfYYEjq-W7Rv",
        "outputId": "22c0b7a5-4e32-45d5-fa10-4a75882fde34"
      },
      "outputs": [],
      "source": [
        "plt.plot(new_table['datetime'],new_table.mean(axis = 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSCCJ46kYnfL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_array_ops import fake_quant_with_min_max_vars_per_channel\n",
        "#we need to create \"windows\" in the data --> \n",
        "#X: input tensor : use a window of size s, lets say 5, as X\n",
        "#Y: output vector : traffic for hour six predicted based on the previous window \n",
        "\n",
        "#this is the window function for multiple input features and multiple output pairs\n",
        "#what's the difference between df_to_x_y2 and df_to_x_y --> the first one is just for one input vector (ie the data from one street) this \n",
        "#version accepts many columns of data (all the street values and the transformed time data)\n",
        "def df_to_x_y2(positional_data,non_positional, window_size): \n",
        "  #df_as_np = df.to_numpy()\n",
        "  print(positional_data.shape)\n",
        "  print(non_positional.shape)\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range((len(positional_data))-window_size-1):\n",
        "    row = [r for r in positional_data[i:i+window_size]]\n",
        "    \n",
        "    #The line below makes the last value in the batch 0 (relative to what we are predicting, \n",
        "    #traffic flow rate, the positional information is still encoded )\n",
        "    row.append(positional_data[i+window_size+1] - non_positional[i+window_size+1]) \n",
        "    X.append(row)\n",
        "    \n",
        "    label = [non_positional[i+window_size][0:]] \n",
        "    Y.append(label)\n",
        "  return np.array(X), np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "w9ddvo5P3o_p",
        "outputId": "d7467aff-3123-43d1-d32f-edcabf5d9bfd"
      },
      "outputs": [],
      "source": [
        "#this is for transforming datetime to recognizable inputs \n",
        "#sin and cos transformation for hour of the day \n",
        "df = new_table\n",
        "df = df[STREETS + ['datetime']] \n",
        " #this removes all of the streets with aveage flow <10 \n",
        "\n",
        "df.head()\n",
        "\n",
        "time = df['datetime'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwhYU7pOk63h"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(df):\n",
        "  import datetime as dt\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  df = pd.Series(df)\n",
        "  hour = df.dt.hour.to_numpy().astype('float32')\n",
        "  hour_sin = np.sin(hour*2* np.pi/23)\n",
        "  hour_cos = np.cos(hour*2* np.pi/23)\n",
        "  \n",
        "  day_of_week = df.dt.dayofweek.to_numpy()\n",
        "  day_of_week_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "  day_of_week = day_of_week.reshape(-1, 1)\n",
        "  day_of_week = day_of_week_scaler.fit_transform(day_of_week)\n",
        "  day_of_week = day_of_week[:,0]\n",
        "  \n",
        "  pos_encoding_val = hour_sin + hour_cos + day_of_week\n",
        "  pos_encoding_val = pos_encoding_val.reshape(-1,1)\n",
        "  final_scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  pos_encoding_val = final_scaler.fit_transform(pos_encoding_val)\n",
        "  return pos_encoding_val\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test positoinal_encoding function \n",
        "positional_encoding(time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, date_time_info,time_weight):\n",
        "    super().__init__()\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    self.time_weight = time_weight\n",
        "    self.scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    self.pos_encoding = positional_encoding(date_time_info)\n",
        "\n",
        "  def call(self, x):\n",
        "    print(x.shape)\n",
        "    print(type(x))\n",
        "    print(x)\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.scaler.fit_transform(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    print(self.pos_encoding.shape)\n",
        "    x_positioned = np.add(x*(1-self.time_weight), (self.time_weight * self.pos_encoding))\n",
        "    print(x.shape)\n",
        "    x_final,y_final= df_to_x_y2(x_positioned,x,window_size=12) \n",
        "    x_final = tf.constant(x_final)\n",
        "    y_final = tf.constant(y_final)\n",
        "    return x_final,y_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#testing the positional embedding class \n",
        "positional_em = PositionalEmbedding(time,0.4)\n",
        "positional_em2 = PositionalEmbedding(time,0)\n",
        "\n",
        "print('shape1',positional_em(df[STREETS].to_numpy())[0][0])\n",
        "print('shape2',positional_em2(df[STREETS].to_numpy())[0][0])\n",
        "\n",
        "x1_df = df[STREETS + ['datetime']].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVOaXVRCobl-"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paapuDkwCKvd"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "   \n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHxIK9yabRL1"
      },
      "outputs": [],
      "source": [
        "#in globalself attention, we are comparing every token to iteself within the same sequence of tokens, here the query and value are both = x\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_eeO4jojMHf"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRlFJ1xynH5h"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "    \n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "      num_heads=num_heads,\n",
        "      key_dim=d_model,\n",
        "      dropout=dropout_rate\n",
        "    )\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    print('eh',x.shape)\n",
        "    x = self.self_attention(x)\n",
        "    print(x.shape)\n",
        "    x = self.causal_self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1_df_test = x1_df[0:336]\n",
        "x1_df_test_time = x1_df_test[:,-1]\n",
        "x1_test_data = df[STREETS][0:336].to_numpy()\n",
        "print()\n",
        "positional_em = PositionalEmbedding(x1_df_test_time,0.4)\n",
        "x1_test, y1_test = positional_em(x1_test_data)\n",
        "print(x1_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(\n",
        "                    d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff\n",
        "                     )\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    print('try embedd',x.shape)\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      print()\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_encoder = Encoder(num_layers=2,\n",
        "                         d_model=keys_dim,\n",
        "                         num_heads=5,\n",
        "                         dff=keys_dim\n",
        "                         )\n",
        "\n",
        "sample_encoder_output = sample_encoder(encoder_test, training=False)\n",
        "\n",
        "# Print the shape.\n",
        "print(encoder_test.shape)\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY5N-ebf6pBe"
      },
      "outputs": [],
      "source": [
        "#encoder only transformer \n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           dropout_rate=dropout_rate)\n",
        "    #flattening before the final layer \n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.final_layer = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    print('input_shape',inputs.shape)\n",
        "    x = inputs\n",
        "    x = self.encoder(x)  # (batch_size, target_len, d_model)\n",
        "    \n",
        "\n",
        "    # Final linear layer output.\n",
        "    #x = self.flatten(x)\n",
        "    x = x[:,-1,:]\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    \n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_layers = 2\n",
        "d_model = keys_dim\n",
        "dff = keys_dim\n",
        "num_heads = 5\n",
        "dropout_rate = 0.1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        " \n",
        "    dropout_rate=dropout_rate)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x1_df.shape)\n",
        "length = 2832\n",
        "#80, 10, 10 split from train val test \n",
        "val_step = int(0.1 * length) * 2\n",
        "test_step = int(0.1 * length)\n",
        "\n",
        "val_s = length - val_step\n",
        "test_s = length - test_step\n",
        "\n",
        "x_train = x1_df[:val_s]\n",
        "x_val = x1_df[val_s:test_s]\n",
        "x_test= x1_df[test_s:]\n",
        "\n",
        "\n",
        "\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#positionally encode and divide all data sets into batches \n",
        "train_time = x_train[:,-1]\n",
        "train_feat = x_train[:,:-1]\n",
        "val_time = x_val[:,-1]\n",
        "val_feat = x_val[:,:-1]\n",
        "test_time = x_test[:,-1]\n",
        "test_feat = x_test[:,:-1]\n",
        "print(x_test.shape)\n",
        "print(x_val.shape)\n",
        "print(test_time.shape)\n",
        "\n",
        "train_pos_enc = PositionalEmbedding(train_time,0.4)\n",
        "val_pos_enc = PositionalEmbedding(val_time,0.4)\n",
        "test_pos_enc = PositionalEmbedding(test_time,0.4)\n",
        "\n",
        "\n",
        "x_train,y_train = train_pos_enc(np.asarray(train_feat).astype('float32'))\n",
        "x_val,y_val = val_pos_enc(np.asarray(val_feat).astype('float32'))\n",
        "x_test,y_test = test_pos_enc(np.asarray(test_feat).astype('float32'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(x_train[0,:,2])\n",
        "\n",
        "plt.scatter(12,y_train[0,:,2])\n",
        "print(y_train[0,:,2])\n",
        "print(x_train[0,:,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(64)\n",
        "val_dataloader = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(10000).batch(64)\n",
        "test_dataloader = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
        "\n",
        "train_batch_size = 2253\n",
        "val_batch_size = 270\n",
        "test_batch_size = 270\n",
        "\n",
        "print(train_dataloader)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model, log=False):\n",
        "    \n",
        "    for traffic_batch, labels_batch in dataloader:\n",
        "        traffic = traffic_batch\n",
        "        print(traffic.shape)\n",
        "        predict_flow = model(traffic)\n",
        "        mse = tf.keras.losses.MeanSquaredError()\n",
        "        mse = mse(labels_batch,predict_flow ).numpy()\n",
        "    \n",
        "    return predict_flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "evaluate(test_dataloader,transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(y_true, y_pred):\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    y = mse(y_true, y_pred)\n",
        "    return y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer.compile(optimizer=optimizer, \n",
        "                  loss = tf.keras.losses.MeanAbsoluteError(), \n",
        "                  metrics= [tf.keras.losses.MeanAbsoluteError()]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer.fit(train_dataloader,epochs = 5,validation_data = val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = evaluate(test_dataloader,transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = transformer.predict(x_test)\n",
        "print(y_test.shape)\n",
        "res[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plotting specific streets \n",
        "plt.rcParams[\"figure.figsize\"] = (20,4)\n",
        "plt.plot(np.arange(300,400),((y_test[80,:,:][0][300:400])),label = 'Actual Values')\n",
        "plt.xlabel('Street Identifier')\n",
        "plt.ylabel('Traffic Conjestion Rate, Scaled Between (0-1)')\n",
        "plt.title('Predicted vs Actual Conjestion Rates for input batch #80')\n",
        "\n",
        "plt.plot(np.arange(300,400),(res[80][300:400]), label = 'Prediction')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checking performance for low actal rates and high actual rates \n",
        "plt.rcParams[\"figure.figsize\"] = (20,4)\n",
        "high_volume = tf.reduce_mean(y_test,2)\n",
        "high_volume = pd.DataFrame(high_volume.numpy(),columns = ['flow_rate'])\n",
        "def is_high_vol(row,vol):\n",
        "    if row['flow_rate'] <= vol:\n",
        "        return 'low'\n",
        "    return 'high'\n",
        "high_volume['h/l'] = high_volume.apply(lambda x: is_high_vol(x,average_flow),axis = 1)\n",
        "high_volume_batches = (high_volume.index[high_volume['h/l']=='high'] ).tolist()\n",
        "high_volume_batches\n",
        "low_volume_batches = (high_volume.index[high_volume['h/l']=='low'] ).tolist()\n",
        "\n",
        "#actual low and high averages \n",
        "high_vol_y_test = tf.gather(y_test,high_volume_batches)\n",
        "low_vol_y_test = tf.gather(y_test,low_volume_batches)\n",
        "low_ave = tf.reduce_mean(low_vol_y_test,0)\n",
        "low_ave_select = low_ave[0][0:150]\n",
        "high_ave = tf.reduce_mean(high_vol_y_test,0)\n",
        "high_ave_select = high_ave[0][0:150]\n",
        "\n",
        "#prediction high and low averages\n",
        "high_vol_pred = tf.gather(res,high_volume_batches)\n",
        "low_vol_pred = tf.gather(res,low_volume_batches)\n",
        "low_ave_pred = tf.reduce_mean(low_vol_pred,0)\n",
        "print(low_ave_pred.shape)\n",
        "low_ave_pred_select = low_ave_pred[0:150]\n",
        "high_ave_pred = tf.reduce_mean(high_vol_pred,0)\n",
        "high_ave_pred_select = high_ave_pred[0:150]\n",
        "\n",
        "plt.plot(scale(high_ave_select), label = 'Actual')\n",
        "plt.plot(scale(high_ave_pred_select),label = 'Prediction')\n",
        "plt.xlabel('Street Identifier')\n",
        "plt.ylabel('Traffic Conjestion Rate, Scaled Between (0-1)')\n",
        "plt.title('Batches with normalized average flow >= average_flow (Prediction and Actual values normalized between 0 and 1)')\n",
        "\n",
        "\n",
        "plt.legend(loc = 'upper right')\n",
        "print(high_volume_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#calculating mean_abs_error for high vs low samples \n",
        "#high \n",
        "high_ave_select2 = high_ave[0][0:]\n",
        "high_ave_pred_select2 = high_ave_pred\n",
        "high_mae = tf.keras.losses.MeanAbsoluteError()\n",
        "h_mae = high_mae(high_ave_select2, high_ave_pred_select2).numpy()\n",
        "\n",
        "#low\n",
        "low_ave_select2 = low_ave[0][0:]\n",
        "low_ave_pred_select2 = low_ave_pred\n",
        "low_mae = tf.keras.losses.MeanAbsoluteError()\n",
        "l_mae = low_mae(low_ave_select2, low_ave_pred_select2).numpy()\n",
        "\n",
        "print(h_mae,l_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale(input):\n",
        "    return tf.divide(tf.subtract(input,tf.reduce_min(input)),tf.subtract(tf.reduce_max(input),tf.reduce_min(input)))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res2 = tf.reduce_mean(res,1)\n",
        "actual2 = tf.reduce_mean(y_test,2)\n",
        "res2_scaled = tf.divide(tf.subtract(res2,tf.reduce_min(res2)),tf.subtract(tf.reduce_max(res2),tf.reduce_min(res2)))\n",
        "actual2_scaled = tf.divide(tf.subtract(actual2,tf.reduce_min(actual2)),tf.subtract(tf.reduce_max(actual2),tf.reduce_min(actual2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(res2_scaled)\n",
        "#plt.plot(actual)\n",
        "plt.plot(actual2_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot((res[:,10]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(scale(res[:,1000]))\n",
        "plt.plot(scale(y_test[:,0,1000]))\n",
        "print((y_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#MSE low vs high flow rates \n",
        "average_flow = tf.reduce_mean(tf.reduce_mean(y_test,2),0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "average_flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "39282081ff6844baad5334981d710ee8987f21358ca5c4d02825da922a75398f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
